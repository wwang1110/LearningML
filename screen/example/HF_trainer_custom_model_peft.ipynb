{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kn8IWjD-VZc"
      },
      "source": [
        "### Install and import packages, helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71Rf3IaQ-ULI",
        "outputId": "c875874c-8dd3-41be-fb9e-8d9655aa4cb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.7/174.7 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#!pip install datasets transformers[torch] accelerate bitsandbytes peft==0.6.2 -U -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UkxnD2Fmj2zn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import PretrainedConfig, PreTrainedModel\n",
        "from transformers import BitsAndBytesConfig\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, PeftModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "97U5JLkl-Qy5"
      },
      "outputs": [],
      "source": [
        "def model_size_in_MB(model):\n",
        "  param_size = 0\n",
        "  for param in model.parameters():\n",
        "      param_size += param.nelement() * param.element_size()\n",
        "  buffer_size = 0\n",
        "  for buffer in model.buffers():\n",
        "      buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "  size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "  print('model size: {:.3f}MB'.format(size_all_mb))\n",
        "\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2mjLvwn93l5"
      },
      "source": [
        "### PretrainedConfig, PreTrainedModel and Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sp9dRWy_kP_t"
      },
      "outputs": [],
      "source": [
        "class MLPConfig(PretrainedConfig):\n",
        "    model_type = \"screen_mlp\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_features: int = 256,\n",
        "        num_classes: int = 3,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        self.num_features = num_features\n",
        "        self.num_classes = num_classes\n",
        "        super().__init__(**kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JsTYlzKTIIC-"
      },
      "outputs": [],
      "source": [
        "class MLP(PreTrainedModel):\n",
        "    config_class = MLPConfig\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.fc1 = nn.Linear(config.num_features, config.num_features)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(config.num_features, config.num_classes)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self._no_split_modules = ['fc1', 'fc2']\n",
        "\n",
        "    def forward(self, input_ids, labels=None):\n",
        "        x = self.fc1(input_ids)\n",
        "        x = self.activation(x)\n",
        "        logits = self.fc2(x)\n",
        "        if labels is None:\n",
        "          return {'logits': logits}\n",
        "        else:\n",
        "          loss = self.criterion(logits, labels)\n",
        "          return {'loss': loss, 'logits': logits}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ues1yJL6llFP"
      },
      "outputs": [],
      "source": [
        "class MLPDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, len):\n",
        "        self.len = len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {}\n",
        "        item['input_ids'] = torch.rand(20).float()\n",
        "        item['labels'] = torch.rand(3).float()\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCd10QjD-Dij"
      },
      "source": [
        "### Train and Save custom model using HF Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Wp6DTa7ijIOo"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cuda_amp half precision backend\n"
          ]
        }
      ],
      "source": [
        "train_dataset = MLPDataset(256)\n",
        "val_dataset = MLPDataset(128)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "        output_dir='./checkpoint',\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=1,\n",
        "        per_device_eval_batch_size=1,\n",
        "        report_to='none',\n",
        "        save_strategy='no',\n",
        "        fp16=True,\n",
        "        remove_unused_columns=False\n",
        "    )\n",
        "\n",
        "mlp_config = MLPConfig(20, 3)\n",
        "model = MLP(mlp_config)\n",
        "\n",
        "trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ka60N2-QI8Q5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 256\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 768\n",
            "  Number of trainable parameters = 483\n",
            " 73%|███████▎  | 561/768 [00:01<00:00, 572.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.6752, 'learning_rate': 1.7447916666666666e-05, 'epoch': 1.95}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 92%|█████████▏| 706/768 [00:01<00:00, 640.57it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "100%|██████████| 768/768 [00:01<00:00, 394.69it/s]\n",
            "Configuration saved in ./output\\config.json\n",
            "Model weights saved in ./output\\pytorch_model.bin\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 1.9461, 'train_samples_per_second': 394.644, 'train_steps_per_second': 394.644, 'train_loss': 1.6779466072718303, 'epoch': 3.0}\n"
          ]
        }
      ],
      "source": [
        "trainer.train()\n",
        "model.save_pretrained(\"./output\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr5Z4Zd8XkEI"
      },
      "source": [
        "### Create LoRA Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FGCeWx-OXjLI"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file ./output\\config.json\n",
            "Model config MLPConfig {\n",
            "  \"architectures\": [\n",
            "    \"MLP\"\n",
            "  ],\n",
            "  \"model_type\": \"screen_mlp\",\n",
            "  \"num_classes\": 3,\n",
            "  \"num_features\": 20,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.1\"\n",
            "}\n",
            "\n",
            "loading weights file ./output\\pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing MLP.\n",
            "\n",
            "All the weights of MLP were initialized from the model checkpoint at ./output.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use MLP for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"fc1\", \"fc2\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    modules_to_save=[\"classifier\"],\n",
        ")\n",
        "base_model = MLP.from_pretrained(\"./output\")\n",
        "base_model = prepare_model_for_kbit_training(base_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "V_mxN_--X0E9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cuda_amp half precision backend\n",
            "***** Running training *****\n",
            "  Num examples = 256\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 768\n",
            "  Number of trainable parameters = 1008\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1008 || all params: 1491 || trainable%: 67.61\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 72%|███████▏  | 556/768 [00:01<00:00, 407.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.6732, 'learning_rate': 1.7447916666666666e-05, 'epoch': 1.95}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 97%|█████████▋| 748/768 [00:01<00:00, 454.31it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "100%|██████████| 768/768 [00:01<00:00, 425.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 1.8084, 'train_samples_per_second': 424.673, 'train_steps_per_second': 424.673, 'train_loss': 1.6763070821762085, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "peft_model1 = get_peft_model(base_model, lora_config)\n",
        "print_trainable_parameters(peft_model1)\n",
        "trainer1 = Trainer(\n",
        "        model=peft_model1,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset\n",
        "    )\n",
        "trainer1.train()\n",
        "peft_model1.save_pretrained('./lora1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nBhccJE9YNs1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cuda_amp half precision backend\n",
            "***** Running training *****\n",
            "  Num examples = 256\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 768\n",
            "  Number of trainable parameters = 1008\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1008 || all params: 1491 || trainable%: 67.61\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 74%|███████▎  | 566/768 [00:01<00:00, 438.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.6732, 'learning_rate': 1.7447916666666666e-05, 'epoch': 1.95}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 99%|█████████▊| 757/768 [00:01<00:00, 465.04it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "100%|██████████| 768/768 [00:01<00:00, 452.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 1.6972, 'train_samples_per_second': 452.515, 'train_steps_per_second': 452.515, 'train_loss': 1.6762625376383464, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "peft_model2 = get_peft_model(base_model, lora_config)\n",
        "print_trainable_parameters(peft_model2)\n",
        "trainer2 = Trainer(\n",
        "        model=peft_model2,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset\n",
        "    )\n",
        "trainer2.train()\n",
        "peft_model1.save_pretrained('./lora2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uaT8ABdYmN7"
      },
      "source": [
        "### Inference LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Wl2Fw7tdYqUb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file ./output\\config.json\n",
            "Model config MLPConfig {\n",
            "  \"architectures\": [\n",
            "    \"MLP\"\n",
            "  ],\n",
            "  \"model_type\": \"screen_mlp\",\n",
            "  \"num_classes\": 3,\n",
            "  \"num_features\": 20,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.1\"\n",
            "}\n",
            "\n",
            "loading weights file ./output\\pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing MLP.\n",
            "\n",
            "All the weights of MLP were initialized from the model checkpoint at ./output.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use MLP for predictions without further training.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "_IncompatibleKeys(missing_keys=['base_model.model.fc1.base_layer.weight', 'base_model.model.fc1.base_layer.bias', 'base_model.model.fc1.lora_A.adapter1.weight', 'base_model.model.fc1.lora_B.adapter1.weight', 'base_model.model.fc2.base_layer.weight', 'base_model.model.fc2.base_layer.bias', 'base_model.model.fc2.lora_A.adapter1.weight', 'base_model.model.fc2.lora_B.adapter1.weight'], unexpected_keys=[])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_model = MLP.from_pretrained(\"./output\", device_map='auto')\n",
        "inf_model = PeftModel.from_pretrained(base_model, './lora1', adapter_name=\"adapter1\", device_map='auto')\n",
        "inf_model.load_adapter('./lora2', adapter_name=\"adapter2\", device_map='auto')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "YleMbMnvZdUD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'logits': tensor([[-0.0892,  0.1122, -0.0155],\n",
            "        [ 0.0974,  0.0344,  0.0315]], device='cuda:0')}\n"
          ]
        }
      ],
      "source": [
        "inputs1 = {'input_ids' : torch.rand(2, 20).float().to(\"cuda:0\")}\n",
        "inf_model.set_adapter(\"adapter1\")\n",
        "\n",
        "for x in inf_model.parameters():\n",
        "  x.requires_grad = False\n",
        "inf_model.eval()\n",
        "output = inf_model(**inputs1)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "a2a0AuzCZk2G"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'logits': tensor([[0.0214, 0.0763, 0.1080],\n",
            "        [0.0904, 0.1173, 0.1598]], device='cuda:0')}\n"
          ]
        }
      ],
      "source": [
        "inputs2 = {'input_ids' : torch.rand(2, 20).float().to(\"cuda:0\")}\n",
        "inf_model.set_adapter(\"adapter2\")\n",
        "\n",
        "for x in inf_model.parameters():\n",
        "  x.requires_grad = False\n",
        "inf_model.eval()\n",
        "output = inf_model(**inputs2)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIbxDRwq-lvK"
      },
      "source": [
        "### Create QLoRA Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nyBF9uaZs0v"
      },
      "outputs": [],
      "source": [
        "#bnb_config = BitsAndBytesConfig(\n",
        "#    load_in_8bit = True,\n",
        "#)\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "   load_in_4bit=True,\n",
        "   bnb_4bit_quant_type=\"nf4\",\n",
        "   bnb_4bit_use_double_quant=True,\n",
        "   bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vlqw0GsAJdV-"
      },
      "outputs": [],
      "source": [
        "model_nf4 = MLP.from_pretrained(\"./output\", quantization_config=nf4_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvyJTRdyLDC1"
      },
      "outputs": [],
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"fc1\", \"fc2\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    modules_to_save=[\"classifier\"],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqCEfcMx-w-s"
      },
      "source": [
        "### Train and Save QLoRA model using HF Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjF7-_DafEAs"
      },
      "outputs": [],
      "source": [
        "peft_model1 = get_peft_model(model, lora_config)\n",
        "print_trainable_parameters(peft_model1)\n",
        "trainer1 = Trainer(\n",
        "        model=peft_model1,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset\n",
        "    )\n",
        "trainer1.train()\n",
        "peft_model1.save_pretrained('./lora1')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZkdGc9Do2Sm"
      },
      "outputs": [],
      "source": [
        "peft_model2 = get_peft_model(model, lora_config)\n",
        "print_trainable_parameters(peft_model2)\n",
        "trainer2 = Trainer(\n",
        "        model=peft_model2,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset\n",
        "    )\n",
        "trainer2.train()\n",
        "peft_model2.save_pretrained('./lora2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgC3GDSY-2mY"
      },
      "source": [
        "### Inference using LoRA Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POyWk4Oso_b9"
      },
      "outputs": [],
      "source": [
        "inf_nf4_config = BitsAndBytesConfig(\n",
        "      load_in_4bit=True,\n",
        "      bnb_4bit_compute_dtype=torch.float32,\n",
        "      bnb_4bit_use_double_quant=True,\n",
        "      bnb_4bit_quant_type='nf4'\n",
        ")\n",
        "\n",
        "base_model = MLP.from_pretrained(\"./output\",\n",
        "                                load_in_4bit=True,\n",
        "                                torch_dtype=torch.float32,\n",
        "                                quantization_config=inf_nf4_config, device_map='auto')\n",
        "inf_model = PeftModel.from_pretrained(base_model, './lora1', adapter_name=\"adapter1\", device_map='auto')\n",
        "\n",
        "inf_model.load_adapter('./lora2', adapter_name=\"adapter2\", device_map='auto')\n",
        "inf_model.to(\"cuda:0\") #not sure why load_adapter will load the lora to difference devices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAQLQxitzOnz"
      },
      "outputs": [],
      "source": [
        "inputs1 = {'input_ids' : torch.rand(2, 20).float().to(\"cuda:0\")}\n",
        "inf_model.set_adapter(\"adapter1\")\n",
        "for x in inf_model.parameters():\n",
        "  x.requires_grad = False\n",
        "inf_model.eval()\n",
        "output = inf_model(**inputs1)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fs4Zv_LfI4sW"
      },
      "outputs": [],
      "source": [
        "inputs2 = {'input_ids' : torch.rand(2, 20).float().to(\"cuda:0\")}\n",
        "inf_model.set_adapter(\"adapter2\")\n",
        "for x in inf_model.parameters():\n",
        "  x.requires_grad = False\n",
        "inf_model.eval()\n",
        "output = inf_model(**inputs2)\n",
        "print(output)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
